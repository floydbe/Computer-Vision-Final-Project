\section{Design and Implementation}
\label{sec:DesignImplementation}
\subsection{Algorithmic Design}
\label{sec:AlgorithmicDesign}
\subsubsection{Definitions}
\label{sec:Definitions}
A \textit{frame} is a two-dimensional array of pixels and a \textit{video} is a sequence of frames. Depending on the type of video, the frames may be grayscale (in which case each pixel contains a single scalar value) or color (in which case pixels contain multiple values that represent color channels). A video can be indexed with a particular index $i$ to retrieve the $i^{th}$ frame as long as $i$ is less than $l$, the total number of frames in the video. 
An \textit{approximate frame} $f'$ is a frame derived from another frame $f$. $f'$'s pixel values are related to $f$'s pixel values according to some function. Depending on the function, $f'$ may contain less or as much information as $f$. As the name implies, however, it is often the case that $fâ€™$ contains less information than f. 
Let $D$ be a function that takes an input video $v$ and two indexes $x$ and $y$ and returns a scalar result. The result of $D(v, x,y)$ is the distance between the frames $x$ and $y$ in the video $v$. There are many different possible ways to define $D$ and calculate the distance. Our definition is based on Euclidean distance and outlined below in detail.
We define a \textit{video sequence} as a pair of indices $(x,y)$ where $x$ is greater than $y$ and $x$ and $y$ are less than $l$. A video sequence $(x,y)$ for video $v$ is \textit{well-looping} if $D(v,x,y) < T$ where $T$ is a threshold value. A \textit{video loop} is a well-looping video sequence $(x,y)$ for video $v$.
\subsubsection{Masks}
\label{sec:Masks}
Masks can be used to filter or replace pixel values in the frames of a video \cite{Piccardi2004}. For example, a mask can filter out all the blue in the frames of a video or replace all pixels outside a particular region with pixels from an external image. A mask might also indicate the region of frames where interesting motion exists throughout a video. 
In the scope of this project, we define the mask as a single two-dimensional array with the same height and width as the frames of the video. The mask may also have associated data (used to make filtering decisions, define replace values, etc). 
Masks can serve two purposes in building video loops: functional and aesthetic. Functionally, masks may help the loop detection algorithm stabilize a shaky input video. A mask could define the moving parts of the video and build approximate frames from that subregion. A user could define a mask as a hint to the loop detection algorithm. We did not implement functional masks in the current version of the system.
Aesthetic masks affect the look of the video produced by the system. For example, they could be used to define the regions of the frames that should be replaced with a solid color or cropped out entirely. We implemented two different aesthetic masks: colorize and background. The colorize mask is used to define a region of pixels of a frame that should retain their original color values -- pixels outside the region will be converted to grayscale. The background mask defines a region of pixels of a frame that should be replaced with pixel values from some external image. 
In the system, masks can be defined according to the median and mean values of a pixel across a video. Masks are applied to frames depending on the difference between the actual value of a pixel and the median (mean) value of that pixel throughout the video. As mentioned above, each mask defines a way to alter the pixel value when the difference exceeds the threshold. 
\subsection{Implementation Design}
\label{sec:ImplementationDesign}
The system is implemented in Python and uses a set of external libraries. 
\subsubsection{Video}
\label{sec:Video}
A \textit{video} is implemented as a \texttt{Video} object. Each video object has certain properties (like the name of the source video file if it was created from a file stored on disk, the total number of frames, etc) and certain methods. 
The frames of the video are stored as an array within the video object. Each frame of a video is accessible through the \texttt{[i]} operator where $i$ is the frame index. The result of the \texttt{[i]} operation is a frame -- a two-dimensional array whose individual objects are formatted according to the type of video. The \texttt{[i:j]} operator, where $i$ and $j$ are frame indexes and $j>i$, creates a new \texttt{Video} whose contents are the frames from $i$ to $j$, inclusive. 
A \texttt{Video} knows how to turn itself into an animated GIF and store the output into a file. A video also knows how to iterate through itself in an idiomatic Python way. 
There are three subclasses of \texttt{Video} (\texttt{GrayVideo}, \texttt{EdgeVideo} and \texttt{ScaleVideo}) that can be instantiated from existing \texttt{Video} objects or from videos stored in files on disk. When a \texttt{GrayVideo} is instantiated, the frames are immediately converted to grayscale. When an \texttt{EdgeVideo} is instantiated, its frames are immediately run through a Canny edge detector. When a \texttt{ScaleVideo} is instantiated, its frames are immediately scaled to a new size. By instantiating one of these subclasses from an existing \texttt{Video} object, a video of approximate frames is immediately available. It is possible to ``chain'' instantiations so that the approximations can be applied one-after-another. 
